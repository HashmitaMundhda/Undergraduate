{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "93a86ed6-eb89-41cb-8456-2c421f8cc953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import these two modules bs4 for selecting HTML tags easily\n",
    "from bs4 import BeautifulSoup\n",
    "# requests module is easy to operate some people use urllib but I prefer this one because it is easy to use.\n",
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "09e754a6-be58-40c5-8989-c66581a1f77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is with title html tag: Top 7 Python Web Scraping Tools For Data Scientists\n",
      "This is h2 html tag: Download our Mobile App\n",
      "This is h3 html tag: 1| Beautiful Soup\n",
      "This is h4 html tag: Subscribe to our newsletter\n",
      "This is h5 html tag: Join our editors every weekday evening as they steer you through the most significant news of the day.\n",
      "<a class=\"skip-link screen-reader-text\" href=\"#content\">Skip to content</a>\n"
     ]
    }
   ],
   "source": [
    "# I put here my own blog url ,you can change it.\n",
    "url=\"https://analyticsindiamag.com/top-7-python-web-scraping-tools-for-data-scientists/\"\n",
    "\n",
    "#Requests module use to data from given url\n",
    "source=requests.get(url)\n",
    "\n",
    "# BeautifulSoup is used for getting HTML structure from requests response.(craete your soup)\n",
    "soup=BeautifulSoup(source.text,'html')\n",
    "\n",
    "# Find function is used to find a single element if there are more than once it always returns the first element.\n",
    "title=soup.find('title') # place your html tagg in parentheses that you want to find from html.\n",
    "print(\"This is with title html tag:\",title.text)\n",
    "\n",
    "qwery=soup.find('h2') # here i find first h2 tagg in my website using find operation.\n",
    "print(\"This is h2 html tag:\",qwery.text) \n",
    "\n",
    "qwery=soup.find('h3') # here i find first h3 tagg in my website using find operation.\n",
    "print(\"This is h3 html tag:\",qwery.text) \n",
    "\n",
    "qwery=soup.find('h4') # here i find first h4 tagg in my website using find operation.\n",
    "print(\"This is h4 html tag:\",qwery.text) \n",
    "\n",
    "qwery=soup.find('h5') # here i find first h5 tagg in my website using find operation.\n",
    "print(\"This is h5 html tag:\",qwery.text) \n",
    "\n",
    "links=soup.find('a') #i extarcted link using \"a\" tag\n",
    "print(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b233c6c-f3b3-4a3e-a27e-26490102fc9b",
   "metadata": {},
   "source": [
    "### Extarct data from Inner HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "11579d17-2654-447d-b0be-3212968eb80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total links in my website : 121\n",
      "\n",
      "<a class=\"skip-link screen-reader-text\" href=\"#content\">Skip to content</a>\n",
      "<a class=\"elementor-item\" data-wpel-link=\"internal\" href=\"https://analyticsindiamag.com/about/\">About</a>\n",
      "<a class=\"elementor-item\" data-wpel-link=\"internal\" href=\"https://analyticsindiamag.com/advertise-with-us/\">Advertise</a>\n",
      "<a class=\"elementor-item\" data-wpel-link=\"internal\" href=\"https://analyticsindiamag.com/our-events/\">Conferences</a>\n",
      "<a class=\"elementor-item\" data-wpel-link=\"external\" href=\"https://aimresearch.co/\" rel=\"follow\" target=\"_blank\">Research</a>\n",
      "<a class=\"elementor-item\" data-wpel-link=\"external\" href=\"https://discord.gg/asWCGawaak\" rel=\"follow\" target=\"_blank\">Community</a>\n"
     ]
    }
   ],
   "source": [
    "# findall function is used to fetch all tags at a single time.\n",
    "many_link=soup.find_all('a') # here i extracted all the anchor tags of my website\n",
    "total_links=len(many_link) # len function is use to calculate length of your array\n",
    "print(\"total links in my website :\",total_links)\n",
    "print()\n",
    "for i in many_link[:6]: # here i use slicing to fetch only first 6 links from rest of them.\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "825c86fe-cd8f-48d1-a788-a80219618326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a class=\"elementor-item\" data-wpel-link=\"internal\" href=\"https://analyticsindiamag.com/about/\">About</a>\n",
      "href is : https://analyticsindiamag.com/about/\n"
     ]
    }
   ],
   "source": [
    "second_link=many_link[1] #here i fetch second link which place on 1 index number in many_links.\n",
    "print(second_link)\n",
    "print(\"href is :\",second_link['href']) #only href link is extracted from ancor tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a58f32b-8260-406c-88bc-d750a80bd74e",
   "metadata": {},
   "source": [
    "### Scrap data from wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7f76893e-36f5-40db-8c67-283aefe192bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Wikipedia - Wikipedia</title>\n"
     ]
    }
   ],
   "source": [
    "wiki=requests.get(\"https://en.wikipedia.org/wiki/Wikipedia\")\n",
    "soup=BeautifulSoup(wiki.text,'html')\n",
    "print(soup.find('title'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e243a951-4e6d-4af5-b052-1167b0876965",
   "metadata": {},
   "source": [
    "### Find html tags with classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cf53b53d-5c8d-4d0e-bb99-71dc221d164b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WikipediaThe logo of Wikipedia, a globe featuring glyphs from various writing systemsScreenshot\n",
      "Wikipedia's desktop homepageType of siteOnline encyclopediaAvailable in335 languagesCountry of originUnited StatesOwnerWikimedia FoundationCreated byJimmy WalesLarry Sanger[1]URLwikipedia.orgCommercialNoRegistrationOptional[note 1]Users>287,402 active editors[note 2]>109,650,061 registered usersLaunchedJanuary 15, 2001(22 years ago) (2001-01-15)Current statusActiveContent licenseCC Attribution / Share-Alike 4.0Most text is also dual-licensed under GFDL; media licensing variesWritten inLAMP platform[2]OCLC number52075003 \n",
      "External video Jimmy Wales, The Birth of Wikipedia, 2006, TED talks, 20 minutes Katherine Maher, What Wikipedia Teaches Us About Balancing Truth and Beliefs, 2022, TED talks, 15 minutes\n",
      "External audio The Great Book of Knowledge, Part 1, Ideas with Paul Kennedy, CBC, January 15, 2014\n",
      "External video Inside Wikipedia – Attack of the PR Industry, Deutsche Welle, 7:13 mins[201]\n"
     ]
    }
   ],
   "source": [
    "overview=soup.find_all('table',class_='infobox')\n",
    "for z in overview:\n",
    "    print(z.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3ca9e91c-4505-4aa0-86ad-ea91afa49dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WikipediaThe logo of Wikipedia, a globe featuring glyphs from various writing systemsScreenshot\n",
      "Wikipedia's desktop homepageType of siteOnline encyclopediaAvailable in335 languagesCountry of originUnited StatesOwnerWikimedia FoundationCreated byJimmy WalesLarry Sanger[1]URLwikipedia.orgCommercialNoRegistrationOptional[note 1]Users>287,402 active editors[note 2]>109,650,061 registered usersLaunchedJanuary 15, 2001(22 years ago) (2001-01-15)Current statusActiveContent licenseCC Attribution / Share-Alike 4.0Most text is also dual-licensed under GFDL; media licensing variesWritten inLAMP platform[2]OCLC number52075003 \n"
     ]
    }
   ],
   "source": [
    "overview=soup.find_all('table',class_='infobox vcard')\n",
    "for z in overview:\n",
    "    print(z.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b5b4d826-be9d-42db-9218-e70840cc92b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vteWikipediaOverview(outline)\n",
      "Censorship\n",
      "Conflict-of-interest editing\n",
      "political editing incidents\n",
      "Criticism\n",
      "Biases\n",
      "gender\n",
      "geographical\n",
      "ideological\n",
      "racial\n",
      "Deletion of articles\n",
      "deletionism and inclusionism\n",
      "notability\n",
      "\"Ignore all rules\"\n",
      "MediaWiki\n",
      "Plagiarism\n",
      "Predictions of the project's end\n",
      "Reliability\n",
      "Fact-checking\n",
      "Citation needed\n",
      "Vandalism\n",
      "Community(Wikipedians)\n",
      "Administrators\n",
      "AfroCrowd\n",
      "Arbitration Committee\n",
      "Art+Feminism\n",
      "Bots\n",
      "Lsjbot\n",
      "Edit count\n",
      "List of Wikipedias\n",
      "The Signpost\n",
      "Wikimedian of the Year\n",
      "Wikipedian in residence\n",
      "WikiProject\n",
      "Women in Red\n",
      "Events\n",
      "Edit-a-thon\n",
      "Wiki Conference India\n",
      "Wiki Indaba\n",
      "WikiConference North America\n",
      "Wikimania\n",
      "Wiki Loves\n",
      "Earth\n",
      "Folklore\n",
      "Monuments\n",
      "Pride\n",
      "People(list)\n",
      "Esra'a Al Shafei\n",
      "Florence Devouard\n",
      "Sue Gardner\n",
      "James Heilman\n",
      "Maryana Iskander\n",
      "Dariusz Jemielniak\n",
      "Rebecca MacKinnon\n",
      "Katherine Maher\n",
      "Magnus Manske\n",
      "Ira Brad Matetsky\n",
      "Erik Möller\n",
      "Jason Moore\n",
      "Raju Narisetti\n",
      "Steven Pruitt\n",
      "Larry Sanger\n",
      "María Sefidari\n",
      "Lisa Seitz-Gruwell\n",
      "Rosie Stephenson-Goodknight\n",
      "Lila Tretikov\n",
      "Jimmy Wales\n",
      "\n",
      "History\n",
      "Bomis\n",
      "Nupedia\n",
      "First edit\n",
      "Logo\n",
      "Internet Watch Foundation\n",
      "Scientology\n",
      "Hillsborough disaster Wikipedia posts\n",
      "VisualEditor\n",
      "#1Lib1Ref\n",
      "2021 Wikimedia Foundation actions on the Chinese Wikipedia\n",
      "Controversies\n",
      "Essjay controversy\n",
      "Henryk Batuta hoax\n",
      "Jar'Edo Wens hoax\n",
      "Seigenthaler biography incident\n",
      "Star Trek Into Darkness debate\n",
      "United States congressional staff edits\n",
      "Zhemao hoaxes\n",
      "Coverage\n",
      "American politics\n",
      "Donald Trump\n",
      "COVID-19 pandemic\n",
      "Russian invasion of Ukraine\n",
      "\n",
      "Honors\n",
      "274301 Wikipedia\n",
      "Wikipedia Monument\n",
      "Referencesand analysis\n",
      "Academic studies\n",
      "Bibliography\n",
      "Cultural\n",
      "Films\n",
      "Listen to Wikipedia\n",
      "Wikipediocracy\n",
      "Mobile access\n",
      "Apps\n",
      "QRpedia\n",
      "Wapedia\n",
      "Wikipedia Zero\n",
      "WikiReader\n",
      "Wikiwand\n",
      "Content use\n",
      "DBpedia\n",
      "Depths of Wikipedia\n",
      "Google and Wikipedia\n",
      "Health information\n",
      "Kiwix\n",
      "Science information\n",
      "Wikimedia Enterprise\n",
      "Related\n",
      "Death and Wikipedia\n",
      "The Iraq War: A Historiography of Wikipedia Changelogs\n",
      "LGBT and Wikipedia\n",
      "Magna Carta (An Embroidery)\n",
      "People imprisoned for editing Wikipedia\n",
      "Print Wikipedia\n",
      "Viola angustifolia\n",
      "Wiki rabbit hole\n",
      "Wikimedia Foundation\n",
      "Wikimedia movement\n",
      "Wikipedia for World Heritage\n",
      "Wikipedia in India\n",
      "Wikiracing\n",
      "List of online encyclopedias\n",
      "List of wikis\n",
      "\n",
      " List\n",
      " Category\n",
      "\n"
     ]
    }
   ],
   "source": [
    "overview=soup.find_all('table',class_='nowraplinks hlist mw-collapsible expanded navbox-inner')\n",
    "for z in overview:\n",
    "    print(z.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc94cd3-5937-4607-9111-bf93f8840dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
